#!/usr/bin/env python3
"""
Test Advanced Prompt Evaluation System

Demonstrates the advanced prompt evaluation capabilities including:
- Performance metrics analysis
- A/B testing
- Cost optimization recommendations
- Session analysis
- Comprehensive evaluation reports
"""

import os
import sys
from datetime import datetime, timedelta
from app import create_app, db
from app.models import Trace, Cost, Prompt
from app.services.prompt_evaluator import PromptEvaluator

def create_test_data():
    """Create comprehensive test data for evaluation."""
    
    print("ðŸš€ Creating Test Data for Advanced Evaluation")
    print("=" * 60)
    
    app = create_app()
    
    with app.app_context():
        # Clear existing data
        Trace.query.delete()
        Cost.query.delete()
        db.session.commit()
        
        # Create test prompts
        prompts = [
            {
                "name": "Meeting Summary v1",
                "content": "Generate a concise summary of the meeting transcript: {transcript}",
                "version": "1.0",
                "prompt_type": "meeting_summary"
            },
            {
                "name": "Meeting Summary v2", 
                "content": "Create a detailed meeting summary with key points and action items: {transcript}",
                "version": "2.0",
                "prompt_type": "meeting_summary"
            },
            {
                "name": "Action Items v1",
                "content": "Extract action items from: {transcript}",
                "version": "1.0",
                "prompt_type": "action_items"
            },
            {
                "name": "Action Items v2",
                "content": "Identify and prioritize action items with assignees: {transcript}",
                "version": "2.0",
                "prompt_type": "action_items"
            }
        ]
        
        # Add prompts to database
        for prompt_data in prompts:
            prompt = Prompt.query.filter_by(name=prompt_data["name"]).first()
            if not prompt:
                prompt = Prompt(
                    name=prompt_data["name"],
                    content=prompt_data["content"],
                    version=prompt_data["version"],
                    prompt_type=prompt_data["prompt_type"],
                    creator_id=1  # Default to admin user
                )
                db.session.add(prompt)
        
        db.session.commit()
        
        # Create traces for the last 30 days
        trace_count = 0
        session_id = 1
        
        for day in range(30):
            date = datetime.now() - timedelta(days=day)
            
            # Create 5-10 traces per day
            traces_per_day = 8 if day < 15 else 6  # More recent data
            
            for trace_num in range(traces_per_day):
                # Alternate between prompts
                prompt_name = prompts[trace_num % len(prompts)]["name"]
                
                # Vary performance based on prompt version
                is_v2 = "v2" in prompt_name
                base_success_rate = 0.95 if is_v2 else 0.85
                base_latency = 4000 if is_v2 else 6000
                base_cost = 0.0018 if is_v2 else 0.0022
                
                # Add some randomness
                import random
                success = random.random() < base_success_rate
                latency = base_latency + random.randint(-500, 500)
                cost = base_cost + random.uniform(-0.0005, 0.0005)
                
                # Create trace
                trace = Trace(
                    trace_id=f"test-trace-{day}-{trace_num}",
                    name=f"Test {prompt_name} - Day {day + 1}",
                    status="success" if success else "error",
                    start_time=date.replace(hour=9 + trace_num, minute=30),
                    end_time=date.replace(hour=9 + trace_num, minute=30, second=latency//1000),
                    duration_ms=latency,
                    trace_metadata={
                        "prompt_name": prompt_name,
                        "session_id": f"session-{session_id}",
                        "project": "vertigo",
                        "workflow_type": "meeting_processing"
                    },
                    error_message="" if success else "Sample error for testing",
                    vertigo_operation="meeting_summary",
                    project="vertigo",
                    meeting_id=f"meeting-{day}-{trace_num}"
                )
                
                db.session.add(trace)
                
                # Create cost record
                cost_record = Cost(
                    trace_id=trace.trace_id,
                    model="gpt-4" if is_v2 else "gpt-3.5-turbo",
                    input_tokens=150 + random.randint(-20, 20),
                    output_tokens=50 + random.randint(-10, 10),
                    cost_usd=max(0.0001, cost),
                    timestamp=date.replace(hour=9 + trace_num, minute=30)
                )
                
                db.session.add(cost_record)
                trace_count += 1
                
                # Create new session every 5 traces
                if trace_num % 5 == 4:
                    session_id += 1
        
        db.session.commit()
        print(f"âœ… Created {trace_count} test traces across {session_id} sessions")
        print(f"ðŸ“Š Test data covers {len(prompts)} different prompts")
        print(f"ðŸ“… Data spans the last 30 days")

def test_advanced_evaluation():
    """Test the advanced evaluation capabilities."""
    
    print("\nðŸ” Testing Advanced Prompt Evaluation")
    print("=" * 60)
    
    app = create_app()
    
    with app.app_context():
        evaluator = PromptEvaluator()
        
        # Test 1: Performance metrics for each prompt
        print("\n1ï¸âƒ£ Performance Metrics Analysis:")
        print("-" * 40)
        
        prompts_to_test = ["Meeting Summary v1", "Meeting Summary v2", "Action Items v1", "Action Items v2"]
        
        for prompt_name in prompts_to_test:
            metrics = evaluator.get_prompt_performance(prompt_name, days=30)
            if metrics:
                print(f"\nðŸ“Š {prompt_name}:")
                print(f"   â€¢ Total Calls: {metrics.total_calls}")
                print(f"   â€¢ Success Rate: {metrics.success_rate}%")
                print(f"   â€¢ Avg Latency: {metrics.avg_latency_ms}ms")
                print(f"   â€¢ Avg Cost: ${metrics.avg_cost_usd}")
                print(f"   â€¢ Total Cost: ${metrics.total_cost_usd}")
                print(f"   â€¢ Error Rate: {metrics.error_rate}%")
        
        # Test 2: A/B Testing
        print("\n2ï¸âƒ£ A/B Testing Results:")
        print("-" * 40)
        
        ab_tests = [
            ("Meeting Summary v1", "Meeting Summary v2"),
            ("Action Items v1", "Action Items v2")
        ]
        
        for prompt_a, prompt_b in ab_tests:
            result = evaluator.compare_prompts(prompt_a, prompt_b, days=30)
            if result:
                print(f"\nðŸ”„ {prompt_a} vs {prompt_b}:")
                print(f"   â€¢ {prompt_a}: {result.a_value}% success rate")
                print(f"   â€¢ {prompt_b}: {result.b_value}% success rate")
                print(f"   â€¢ Improvement: {result.improvement}%")
                print(f"   â€¢ Confidence: {result.confidence_level}")
                print(f"   â€¢ Significant: {'âœ…' if result.is_significant else 'âŒ'}")
        
        # Test 3: Cost Optimization Recommendations
        print("\n3ï¸âƒ£ Cost Optimization Recommendations:")
        print("-" * 40)
        
        for prompt_name in prompts_to_test:
            recommendations = evaluator.get_cost_optimization_recommendations(prompt_name)
            if recommendations:
                print(f"\nðŸ’° {prompt_name}:")
                for rec in recommendations:
                    print(f"   â€¢ {rec['priority'].upper()}: {rec['message']}")
                    print(f"     Potential savings: {rec['potential_savings']}")
        
        # Test 4: Session Analysis
        print("\n4ï¸âƒ£ Session Analysis:")
        print("-" * 40)
        
        # Analyze a few sample sessions
        for session_id in range(1, 4):
            session_data = evaluator.get_session_analysis(f"session-{session_id}")
            if session_data:
                print(f"\nðŸ“± Session {session_id}:")
                print(f"   â€¢ Total Calls: {session_data['total_calls']}")
                print(f"   â€¢ Duration: {session_data['session_duration_seconds']:.1f}s")
                print(f"   â€¢ Total Cost: ${session_data['total_cost_usd']}")
                print(f"   â€¢ Success Rate: {session_data['success_rate']}%")
                print(f"   â€¢ Calls/Minute: {session_data['calls_per_minute']:.2f}")
        
        # Test 5: Comprehensive Evaluation Report
        print("\n5ï¸âƒ£ Comprehensive Evaluation Report:")
        print("-" * 40)
        
        for prompt_name in prompts_to_test:
            report = evaluator.generate_evaluation_report(prompt_name, days=30)
            if "error" not in report:
                print(f"\nðŸ“‹ {prompt_name} Evaluation Report:")
                print(f"   â€¢ Grade: {report['grade']}")
                print(f"   â€¢ Total Calls: {report['metrics']['total_calls']}")
                print(f"   â€¢ Success Rate: {report['metrics']['success_rate']}%")
                print(f"   â€¢ Total Cost: ${report['metrics']['total_cost_usd']}")
                print(f"   â€¢ Recommendations: {len(report['recommendations'])}")

def main():
    """Main test function."""
    
    print("ðŸŽ¯ Advanced Prompt Evaluation System Test")
    print("=" * 60)
    
    # Create test data
    create_test_data()
    
    # Run evaluation tests
    test_advanced_evaluation()
    
    print("\nðŸŽ‰ Advanced Evaluation Test Complete!")
    print("=" * 60)
    print("âœ… All evaluation features are working")
    print("ðŸ“Š You can now use these tools to analyze your real prompts")
    print("ðŸ”— The data is ready for integration with your dashboard")

if __name__ == "__main__":
    main() 