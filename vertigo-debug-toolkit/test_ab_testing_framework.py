#!/usr/bin/env python3
"""
A/B Testing Framework Integration Test
Tests the complete A/B testing functionality including test creation, variant generation, 
traffic splitting, result recording, and statistical analysis.
"""

import sys
import json
import time
import random
from datetime import datetime
from app import create_app, db
from app.services.ml_optimization.ab_testing_orchestrator import (
    ab_testing_orchestrator, ABTestConfiguration, TrafficSplit
)
from app.services.ml_optimization.prompt_variant_generator import (
    prompt_variant_generator, VariantGenerationConfig
)


def test_ab_testing_framework():
    """Test the complete A/B testing framework."""
    app = create_app()
    
    with app.app_context():
        print("üß™ A/B Testing Framework Integration Test")
        print("=" * 60)
        
        # Step 1: Create A/B Test
        print("\n1Ô∏è‚É£ Creating A/B Test...")
        test_config = ABTestConfiguration(
            name="Email Summary Prompt Optimization",
            description="Testing different prompt variants for email summarization",
            hypothesis="Shorter, more structured prompts will reduce latency and cost while maintaining quality",
            success_metrics=["latency", "cost", "success_rate"],
            traffic_splits=[
                TrafficSplit(variant_id="control", percentage=40, is_control=True),
                TrafficSplit(variant_id="variant_1", percentage=30, is_control=False),
                TrafficSplit(variant_id="variant_2", percentage=30, is_control=False)
            ],
            min_sample_size=50,
            confidence_level=0.95,
            max_duration_hours=24,
            auto_conclude=True,
            auto_implement=False
        )
        
        test_id = ab_testing_orchestrator.create_ab_test(test_config, creator_id=1)
        print(f"‚úÖ Created A/B test: {test_id}")
        
        # Step 2: Generate AI-Optimized Variants
        print("\n2Ô∏è‚É£ Generating AI-Optimized Variants...")
        base_prompt = """Please provide a comprehensive summary of the following email, including:
        - Main purpose and key points
        - Action items or requests
        - Important dates or deadlines
        - Priority level (high/medium/low)
        
        Please be thorough and capture all relevant information from the email content."""
        
        variant_config = VariantGenerationConfig(
            base_prompt=base_prompt,
            optimization_targets=["latency", "cost", "clarity"],
            variation_types=["structure", "length", "tone"],
            num_variants=2,
            include_control=False,
            creativity_level=0.7
        )
        
        variants = prompt_variant_generator.generate_variants(variant_config)
        print(f"‚úÖ Generated {len(variants)} prompt variants:")
        
        # Configure variants
        variant_configs = {}
        for i, variant in enumerate(variants):
            variant_id = f"variant_{i+1}" if not variant.is_control else "control"
            variant_configs[variant_id] = {
                'name': variant.name,
                'description': f"Generated using {variant.generation_method}",
                'prompt_content': variant.content,
                'generation_method': variant.generation_method,
                'generation_context': variant.generation_context
            }
            print(f"  - {variant_id}: {variant.name} (confidence: {variant.confidence_score:.1%})")
        
        # Add control variant manually
        variant_configs['control'] = {
            'name': 'Control (Original)',
            'description': 'Original prompt',
            'prompt_content': base_prompt,
            'generation_method': 'manual'
        }
        
        success = ab_testing_orchestrator.configure_variants(test_id, variant_configs)
        print(f"‚úÖ Configured variants: {success}")
        
        # Step 3: Start the Test
        print("\n3Ô∏è‚É£ Starting A/B Test...")
        success = ab_testing_orchestrator.start_test(test_id)
        print(f"‚úÖ Test started: {success}")
        
        # Step 4: Simulate Traffic and Results
        print("\n4Ô∏è‚É£ Simulating Test Traffic and Results...")
        
        # Simulate 60 requests across variants
        for i in range(60):
            user_session = f"user_session_{random.randint(1, 20)}"  # 20 unique users
            
            # Select variant based on traffic split
            selected_variant = ab_testing_orchestrator.select_variant(test_id, user_session)
            
            if selected_variant:
                # Simulate realistic performance metrics based on variant type
                if "control" in selected_variant:
                    latency_ms = random.gauss(2500, 400)  # Control: higher latency
                    cost_usd = random.gauss(0.025, 0.005)  # Control: higher cost
                    success_rate = 0.92
                elif "variant_1" in selected_variant:
                    latency_ms = random.gauss(1800, 300)  # Variant 1: reduced latency
                    cost_usd = random.gauss(0.018, 0.004)  # Variant 1: reduced cost  
                    success_rate = 0.94
                else:  # variant_2
                    latency_ms = random.gauss(2200, 350)  # Variant 2: moderate improvement
                    cost_usd = random.gauss(0.020, 0.004)  # Variant 2: moderate cost
                    success_rate = 0.91
                
                # Determine success based on success rate
                success = random.random() < success_rate
                
                # Ensure positive values
                latency_ms = max(500, latency_ms)
                cost_usd = max(0.001, cost_usd)
                
                # Record result
                ab_testing_orchestrator.record_result(
                    test_id=test_id,
                    variant_id=selected_variant,
                    request_id=f"req_{test_id}_{i:03d}",
                    latency_ms=latency_ms,
                    cost_usd=cost_usd,
                    success=success,
                    user_session=user_session,
                    context={"simulation": True, "batch": i // 20}
                )
        
        print(f"‚úÖ Simulated 60 requests across all variants")
        
        # Step 5: Run Statistical Analysis
        print("\n5Ô∏è‚É£ Running Statistical Analysis...")
        analysis_results = ab_testing_orchestrator.analyze_test(test_id)
        
        if analysis_results['status'] == 'analyzed':
            print("‚úÖ Statistical analysis completed:")
            print(f"  - Sample size: {analysis_results['sample_size']}")
            
            recommendation = analysis_results['recommendation']
            print(f"  - Recommendation: {recommendation['action']}")
            print(f"  - Confidence: {recommendation['confidence']:.1%}")
            print(f"  - Reasoning: {recommendation['reasoning']}")
            
            if recommendation.get('winning_variant_id'):
                print(f"  - Winning variant: {recommendation['winning_variant_id']}")
                if recommendation.get('expected_impact'):
                    impact = recommendation['expected_impact'].get('improvement_percent', 0)
                    print(f"  - Expected improvement: {impact:.1f}%")
            
            # Display results for each metric
            for metric, results in analysis_results['analysis_results'].items():
                if 'error' not in results and results.get('overall_significant'):
                    print(f"  - {metric}: Statistically significant differences detected!")
                    
        else:
            print(f"‚ö†Ô∏è  Analysis status: {analysis_results['status']}")
            if 'error' in analysis_results:
                print(f"   Error: {analysis_results['error']}")
        
        # Step 6: Get Comprehensive Test Status
        print("\n6Ô∏è‚É£ Test Status Summary...")
        status_info = ab_testing_orchestrator.get_test_status(test_id)
        
        if 'error' not in status_info:
            print(f"‚úÖ Test Status: {status_info['status']}")
            print(f"  - Progress: {status_info['progress']:.1%}")
            print(f"  - Results collected: {status_info['results_count']}")
            print(f"  - Elapsed time: {status_info['elapsed_hours']:.1f} hours")
            
            print(f"  - Variant Performance:")
            for variant in status_info['variants']:
                print(f"    ‚Ä¢ {variant['name']}: {variant['requests_served']} requests, "
                      f"{variant['success_rate']:.1%} success, "
                      f"{variant['avg_latency_ms']:.0f}ms avg, "
                      f"${variant['avg_cost_usd']:.4f} avg cost")
                      
        # Step 7: Test API Integration (optional simulation)
        print("\n7Ô∏è‚É£ Testing Variant Selection API...")
        for i in range(5):
            variant = ab_testing_orchestrator.select_variant(test_id, f"api_test_user_{i}")
            print(f"  - User {i}: Selected {variant}")
        
        # Step 8: Cleanup (stop test)
        print("\n8Ô∏è‚É£ Stopping Test...")
        ab_testing_orchestrator.stop_test(test_id, "Test completed successfully")
        print("‚úÖ Test stopped")
        
        print("\n" + "=" * 60)
        print("üéâ A/B Testing Framework Integration Test COMPLETED!")
        print("   ‚úÖ Test creation and configuration")
        print("   ‚úÖ AI-powered variant generation")
        print("   ‚úÖ Traffic splitting and selection")
        print("   ‚úÖ Result recording and tracking")
        print("   ‚úÖ Statistical analysis and recommendations")
        print("   ‚úÖ Dashboard integration ready")
        print("=" * 60)
        
        return True


if __name__ == "__main__":
    try:
        test_ab_testing_framework()
        print("\nüöÄ A/B Testing Framework is ready for production use!")
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)